
@techreport{Pew2018,
author = {Smith, Aaron},
title = {Public Attitudes towards Computer Algorithms}, year = {2018},
source = {https://www.pewresearch.org/internet/2018/11/16/public-attitudes-toward-computer-algorithms/},
publisher = {Pew Research Center},
address = {Washington, D.C., USA},
}


@article{martin_what_2020,
	title = {What {Is} {It} {About} {Location}?},
	volume = {35},
	url = {https://papers.ssrn.com/abstract=3360409},
	doi = {10.2139/ssrn.3360409},
	language = {en},
	number = {1},
	urldate = {2023-09-04},
	journal = {Berkeley Technology Law Journal},
	author = {Martin, Kirsten and Nissenbaum, Helen},
	year = {2020},
	keywords = {Helen Nissenbaum, Kirsten Martin, SSRN, What Is It About Location?},
}

@techreport{Fra2020,
author = {European Union Agency for Fundamental Rights},
title = {Your Rights Matter: Data Protection and Privacy: Fundamental Rights Survey.}, year = {2020},
source = {https://fra.europa.eu/en/publication/2020/fundamental-rights-survey-data-protection#publication-tab-0},
publisher = {Publications Office of the European Union},
address = {Luxembourg},
}

@article{gilbert_measuring_2021,
	title = {Measuring {Americans}â€™ {Comfort} {With} {Research} {Uses} of {Their} {Social} {Media} {Data}},
	volume = {7},
	issn = {2056-3051},
	url = {https://doi.org/10.1177/20563051211033824},
	doi = {10.1177/20563051211033824},
	language = {en},
	number = {3},
	urldate = {2023-09-04},
	journal = {Social Media + Society},
	author = {Gilbert, Sarah and Vitak, Jessica and Shilton, Katie},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20563051211033824}
}


@inproceedings{zhang-etal-2021-pairwise,
    title = "Pairwise Supervised Contrastive Learning of Sentence Representations",
    author = "Zhang, Dejiao  and
      Li, Shang-Wen  and
      Xiao, Wei  and
      Zhu, Henghui  and
      Nallapati, Ramesh  and
      Arnold, Andrew O.  and
      Xiang, Bing",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.467",
    doi = "10.18653/v1/2021.emnlp-main.467",
    pages = "5786--5798",
    abstract = "Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10{\%}{--}13{\%} averaged improvement on eight clustering tasks, and 5{\%}{--}6{\%} averaged improvement on seven semantic textual similarity (STS) tasks.",
}

@inproceedings{simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@online{snapchat_2023,
author ={Statista},
year = {2023},
title ={Snapchat daily active users 2023},
url ={https://www.statista.com/statistics/545967/snapchat-app-dau/},
month ={aug},
lastaccessed ={August 29, 2005},
}


@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
	urldate = {2023-08-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\76RZA7ZK\\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}


@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{dse,
    title = "Learning Dialogue Representations from Consecutive Utterances",
    author = "Zhou, Zhihan  and
      Zhang, Dejiao  and
      Xiao, Wei  and
      Dingwall, Nicholas  and
      Ma, Xiaofei  and
      Arnold, Andrew  and
      Xiang, Bing",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.55",
    doi = "10.18653/v1/2022.naacl-main.55",
    pages = "754--768",
    abstract = "Learning high-quality dialogue representations is essential for solving a variety of dialogue-oriented tasks, especially considering that dialogue systems often suffer from data scarcity. In this paper, we introduce Dialogue Sentence Embedding (DSE), a self-supervised contrastive learning method that learns effective dialogue representations suitable for a wide range of dialogue tasks. DSE learns from dialogues by taking consecutive utterances of the same dialogue as positive pairs for contrastive learning. Despite its simplicity, DSE achieves significantly better representation capability than other dialogue representation and universal sentence representation models. We evaluate DSE on five downstream dialogue tasks that examine dialogue representation at different semantic granularities. Experiments in few-shot and zero-shot settings show that DSE outperforms baselines by a large margin, for example, it achieves 13{\%} average performance improvement over the strongest unsupervised baseline in 1-shot intent classification on 6 datasets. We also provide analyses on the benefits and limitations of our model.",
}


@inproceedings{arora-etal-2020-contextual,
    title = "Contextual Embeddings: When Are They Worth It?",
    author = "Arora, Simran  and
      May, Avner  and
      Zhang, Jian  and
      R{\'e}, Christopher",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.236",
    doi = "10.18653/v1/2020.acl-main.236",
    pages = "2650--2663",
    abstract = "We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline{---}random word embeddings{---}focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10{\%} accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.",
}


@misc{retnet,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}


@inproceedings{chen_predictive_2018,
	address = {Torino Italy},
	title = {Predictive {Analysis} by {Leveraging} {Temporal} {User} {Behavior} and {User} {Embeddings}},
	isbn = {978-1-4503-6014-2},
	url = {https://dl.acm.org/doi/10.1145/3269206.3272032},
	doi = {10.1145/3269206.3272032},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Chen, Charles and Kim, Sungchul and Bui, Hung and Rossi, Ryan and Koh, Eunyee and Kveton, Branislav and Bunescu, Razvan},
	month = oct,
	year = {2018},
	pages = {2175--2182},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\ESJRHPMI\\Chen et al. - 2018 - Predictive Analysis by Leveraging Temporal User Be.pdf:application/pdf},
}

@inproceedings{wu_ptum_2020,
	address = {Online},
	title = {{PTUM}: {Pre}-training {User} {Model} from {Unlabeled} {User} {Behaviors} via {Self}-supervision},
	shorttitle = {{PTUM}},
	url = {https://aclanthology.org/2020.findings-emnlp.174},
	doi = {10.18653/v1/2020.findings-emnlp.174},
	abstract = {User modeling is critical for many personalized web services. Many existing methods model users based on their behaviors and the labeled data of target tasks. However, these methods cannot exploit useful information in unlabeled user behavior data, and their performance may be not optimal when labeled data is scarce. Motivated by pre-trained language models which are pre-trained on large-scale unlabeled corpus to empower many downstream tasks, in this paper we propose to pre-train user models from large-scale unlabeled user behaviors data. We propose two self-supervision tasks for user model pre-training. The first one is masked behavior prediction, which can model the relatedness between historical behaviors. The second one is next K behavior prediction, which can model the relatedness between past and future behaviors. The pre-trained user models are finetuned in downstream tasks to learn task-specific user representations. Experimental results on two real-world datasets validate the effectiveness of our proposed user model pre-training method.},
	urldate = {2023-08-18},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Lian, Jianxun and Huang, Yongfeng and Xie, Xing},
	month = nov,
	year = {2020},
	pages = {1939--1944},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\BV23XWK4\\Wu et al. - 2020 - PTUM Pre-training User Model from Unlabeled User .pdf:application/pdf},
}

@inproceedings{zheng2017joint,
  title={Joint deep modeling of users and items using reviews for recommendation},
  author={Zheng, Lei and Noroozi, Vahid and Yu, Philip S},
  booktitle={Proceedings of the tenth ACM international conference on web search and data mining},
  pages={425--434},
  year={2017}
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  booktitle={The world wide web conference},
  pages={417--426},
  year={2019}
}

@inproceedings{liu2010personalized,
  title={Personalized news recommendation based on click behavior},
  author={Liu, Jiahui and Dolan, Peter and Pedersen, Elin R{\o}nby},
  booktitle={Proceedings of the 15th international conference on Intelligent user interfaces},
  pages={31--40},
  year={2010}
}

@inproceedings{andrews_learning_2019,
	address = {Hong Kong, China},
	title = {Learning {Invariant} {Representations} of {Social} {Media} {Users}},
	url = {https://aclanthology.org/D19-1178},
	doi = {10.18653/v1/D19-1178},
	abstract = {The evolution of social media users' behavior over time complicates user-level comparison tasks such as verification, classification, clustering, and ranking. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a mapping from short episodes of user activity on social media to a vector space in which the distance between points captures the similarity of the corresponding users' invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the mapping may be applied to users not seen at training time and enables efficient comparisons of users in the resulting vector space. We present a comprehensive evaluation to validate the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia.},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Andrews, Nicholas and Bishop, Marcus},
	month = nov,
	year = {2019},
	pages = {1684--1695},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\TCIF2N9F\\Andrews and Bishop - 2019 - Learning Invariant Representations of Social Media.pdf:application/pdf},
}

@inproceedings{chen_forum_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Forum {User} {Profiling} by {Incorporating} {User} {Behavior} and {Social} {Network} {Connections}},
	isbn = {978-3-319-94307-7},
	doi = {10.1007/978-3-319-94307-7_3},
	abstract = {With the rapid development of social media in recent years, user profile inferring becomes crucial to many practical application such as recommendation and customized service. In this paper, we propose an ensemble learning based model, which incorporates user behavior embedding and social network connection embedding, for user profile inference. In which, post content features and user behavior statistics are employed to learn the user behavior embedding. LINE and PUHE are incorporated to learn the user social network connection embedding. The proposed method is evaluated on SMP CUP 2017 user profiling competition dataset. The experiment results demonstrate that leveraging both user behavior embedding and social network connection embedding improves the user profiling efficiently.},
	language = {en},
	booktitle = {Cognitive {Computing} â€“ {ICCC} 2018},
	publisher = {Springer International Publishing},
	author = {Chen, Di and Zhang, Qinglin and Chen, Gangbao and Fan, Chuang and Gao, Qinghong},
	editor = {Xiao, Jing and Mao, Zhi-Hong and Suzumura, Toyotaro and Zhang, Liang-Jie},
	year = {2018},
	keywords = {Ensemble learning, Social network, User behavior, User profiling},
	pages = {30--42},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\K9T2U2IA\\Chen et al. - 2018 - Forum User Profiling by Incorporating User Behavio.pdf:application/pdf},
}


@inproceedings{modell_graph_2021,
	title = {A {Graph} {Embedding} {Approach} to {User} {Behavior} {Anomaly} {Detection}},
	doi = {10.1109/BigData52589.2021.9671423},
	abstract = {Identifying suspicious user behavior within an enterprise network is vital to maintaining strong cyber security defenses. This paper presents a scalable approach to detecting anomalous user behavior in event logs, which we frame as a dynamic, bipartite interaction network of users and resources. Graph embedding is used to obtain vector representations of users, which are updated over time and used to model the profile of the users who typically access each resource. A standard nearest neighbor anomaly detection method is then employed to score new interactions. The approach is applied to a dataset of interaction events between users and SharePoint sites within Microsoftâ€™s internal corporate network.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Modell, Alexander and Larson, Jonathan and Turcotte, Melissa and Bertiger, Anna},
	month = dec,
	year = {2021},
	keywords = {Sensitivity, Conferences, Big Data, anomaly detection, cyber security, Dynamic scheduling, graph embedding, Organizations, Robustness, Standards organizations},
	pages = {2650--2655},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\fqixi\\Zotero\\storage\\YNGZCR3Z\\9671423.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\M9K3SWB9\\Modell et al. - 2021 - A Graph Embedding Approach to User Behavior Anomal.pdf:application/pdf}
}

@inproceedings{waller_generalists_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {Generalists and {Specialists}: {Using} {Community} {Embeddings} to {Quantify} {Activity} {Diversity} in {Online} {Platforms}},
	isbn = {978-1-4503-6674-8},
	shorttitle = {Generalists and {Specialists}},
	url = {https://dl.acm.org/doi/10.1145/3308558.3313729},
	doi = {10.1145/3308558.3313729},
	abstract = {In many online platforms, people must choose how broadly to allocate their energy. Should one concentrate on a narrow area of focus, and become a specialist, or apply oneself more broadly, and become a generalist? In this work, we propose a principled measure of how generalist or specialist a user is, and study behavior in online platforms through this lens. To do this, we construct highly accurate community embeddings that represent communities in a high-dimensional space. We develop sets of community analogies and use them to optimize our embeddings so that they encode community relationships extremely well. Based on these embeddings, we introduce a natural measure of activity diversity, the GS-score. Applying our embedding-based measure to online platforms, we observe a broad spectrum of user activity styles, from extreme specialists to extreme generalists, in both community membership on Reddit and programming contributions on GitHub. We find that activity diversity is related to many important phenomena of user behavior. For example, specialists are much more likely to stay in communities they contribute to, but generalists are much more likely to remain on platforms as a whole. We also find that generalists engage with significantly more diverse sets of users than specialists do. Furthermore, our methodology leads to a simple algorithm for community recommendation, matching state-of-the-art methods like collaborative filtering. Our methods and results introduce an important new dimension of online user behavior and shed light on many aspects of online platform use.},
	urldate = {2023-08-18},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Waller, Isaac and Anderson, Ashton},
	month = may,
	year = {2019},
	keywords = {activity diversity, community embeddings, community recommendation, generalist and specialists},
	pages = {1954--1964},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\KYMCJFTY\\Waller and Anderson - 2019 - Generalists and Specialists Using Community Embed.pdf:application/pdf},
}


@inproceedings{zhang_general-purpose_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {General-{Purpose} {User} {Embeddings} based on {Mobile} {App} {Usage}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403334},
	doi = {10.1145/3394486.3403334},
	abstract = {In this paper, we report our recent practice at Tencent for user modeling based on mobile app usage. User behaviors on mobile app usage, including retention, installation, and uninstallation, can be a good indicator for both long-term and short-term interests of users. For example, if a user installs Snapseed recently, she might have a growing interest in photographing. Such information is valuable for numerous downstream applications, including advertising, recommendations, etc. Traditionally, user modeling from mobile app usage heavily relies on handcrafted feature engineering, which requires onerous human work for different downstream applications, and could be sub-optimal without domain experts. However, automatic user modeling based on mobile app usage faces unique challenges, including (1) retention, installation, and uninstallation are heterogeneous but need to be modeled collectively, (2) user behaviors are distributed unevenly over time, and (3) many long-tailed apps suffer from serious sparsity. In this paper, we present a tailored Auto Encoder-coupled Transformer Network (AETN), by which we overcome these challenges and achieve the goals of reducing manual efforts and boosting performance. We have deployed the model at Tencent, and both online/offline experiments from multiple domains of downstream applications have demonstrated the effectiveness of the output user embeddings.},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Junqi and Bai, Bing and Lin, Ye and Liang, Jian and Bai, Kun and Wang, Fei},
	month = aug,
	year = {2020},
	keywords = {user modeling, app usage, autoencoder, embeddings, transformer},
	pages = {2831--2840},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\UFS8JR85\\Zhang et al. - 2020 - General-Purpose User Embeddings based on Mobile Ap.pdf:application/pdf},
}

@inproceedings{sun2022learning,
  title={Learning Interest-oriented Universal User Representation via Self-supervision},
  author={Sun, Qinghui and Gu, Jie and Xu, XiaoXiao and Xu, Renjun and Liu, Ke and Yang, Bei and Liu, Hong and Xu, Huan},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={7270--7278},
  year={2022}
}


@inproceedings{abb_reference_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Reference} {Data} {Model} forÂ {Process}-{Related} {User} {Interaction} {Logs}},
	isbn = {978-3-031-16103-2},
	doi = {10.1007/978-3-031-16103-2_7},
	abstract = {User interaction (UI) logs are high-resolution event logs that record low-level activities performed by a user during the execution of a task in an information system. Each event in a UI log corresponds to a single interaction between the user and the interface, such as clicking a button or entering a string into a text field. UI logs are used for purposes like task mining or robotic process automation (RPA), but each study and tool relies on a different conceptualization and implementation of the elements and attributes that constitute user interactions. This lack of standardization makes it difficult to integrate UI logs from different sources and to combine tools for UI data collection with downstream analytics or automation solutions. To address this, we propose a universally applicable reference data model for process-related UI logs. Based on a review of scientific literature and industry solutions, this model includes the core attributes of UI logs, but remains flexible with regard to the scope, level of abstraction, and case notion. We provide an implementation of the model as an extension to the XES interchange standard for event logs and demonstrate its practical applicability in a real-life RPA scenario.},
	language = {en},
	booktitle = {Business {Process} {Management}},
	publisher = {Springer International Publishing},
	author = {Abb, Luka and Rehse, Jana-Rebecca},
	editor = {Di Ciccio, Claudio and Dijkman, Remco and del RÃ­o Ortega, Adela and Rinderle-Ma, Stefanie},
	year = {2022},
	keywords = {Data model, Robotic process automation, Task mining, UI Log, User behavior mining},
	pages = {57--74},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\LSHK9SJF\\Abb and Rehse - 2022 - A Reference Data Model forÂ Process-Related User In.pdf:application/pdf},
}


@inproceedings{yang_personalizing_2017,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '17 {Companion}},
	title = {Personalizing {Software} and {Web} {Services} by {Integrating} {Unstructured} {Application} {Usage} {Traces}},
	isbn = {978-1-4503-4914-7},
	url = {https://dl.acm.org/doi/10.1145/3041021.3054183},
	doi = {10.1145/3041021.3054183},
	abstract = {Users of software applications generate vast amounts of unstructured log-trace data. These traces contain clues to the intentions and interests of those users, but service providers may find it difficult to uncover and exploit those clues. In this paper, we propose a framework for personalizing software and web services by leveraging such unstructured traces. We use 6 months of Photoshop usage history and 7 years of interaction records from 67K Behance users to design, develop, and validate a user-modeling technique that discovers highly discriminative representations of Photoshop users; we refer to the model as cutilization-to-vector, util2vec. We demonstrate the promise of this approach for three sample applications: (1) a practical user-tagging system that automatically predicts areas of focus for millions of Photoshop users; (2) a two-phase recommendation model that enables cold-start personalized recommendations for many new Behance users who have Photoshop usage data, improving recommendation quality (Recall@100) by 21.2\% over a popularity-based recommender; and (3) a novel inspiration engine that provides real-time personalized inspirations to artists. We believe that this work demonstrates the potential impact of unstructured usage-log data for personalization.},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web} {Companion}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Yang, Longqi and Fang, Chen and Jin, Hailin and Hoffman, Matthew D. and Estrin, Deborah},
	month = apr,
	year = {2017},
	keywords = {user modeling, application usage, recommendation},
	pages = {485--493},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\MZDUA6XD\\Yang et al. - 2017 - Personalizing Software and Web Services by Integra.pdf:application/pdf},
}


@inproceedings{tao_log2intent_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {{Log2Intent}: {Towards} {Interpretable} {User} {Modeling} via {Recurrent} {Semantics} {Memory} {Unit}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {{Log2Intent}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330889},
	doi = {10.1145/3292500.3330889},
	abstract = {Modeling user behavior from unstructured software log-trace data is critical in providing personalized service ({\textbackslash}emphe.g., cross-platform recommendation). Existing user modeling approaches cannot well handle the long-term temporal information in log data, or produce semantically meaningful results for interpreting user logs. To address these challenges, we propose a Log2Intent framework for interpretable user modeling in this paper. Log2Intent adopts a deep sequential modeling framework that contains a temporal encoder, a semantic encoder and a log action decoder, and it fully captures the long-term temporal information in user sessions. Moreover, to bridge the semantic gap between log-trace data and human language, a recurrent semantics memory unit (RSMU) is proposed to encode the annotation sentences from an auxiliary software tutorial dataset, and the output of RSMU is fed into the semantic encoder of Log2Intent. Comprehensive experiments on a real-world Photoshop log-trace dataset with an auxiliary Photoshop tutorial dataset demonstrate the effectiveness of the proposed Log2Intent framework over the state-of-the-art log-trace user modeling method in three different tasks, including log annotation retrieval, user interest detection and user next action prediction.},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tao, Zhiqiang and Li, Sheng and Wang, Zhaowen and Fang, Chen and Yang, Longqi and Zhao, Handong and Fu, Yun},
	month = jul,
	year = {2019},
	keywords = {user modeling, log-trace data, recurrent memory network, semantics attention, sequential modeling},
	pages = {1055--1063},
	file = {Full Text PDF:C\:\\Users\\fqixi\\Zotero\\storage\\CE39FNHL\\Tao et al. - 2019 - Log2Intent Towards Interpretable User Modeling vi.pdf:application/pdf},
}



@inproceedings{chu_simcurl_2022,
	address = {Bahamas},
	title = {{SimCURL}: {Simple} {Contrastive} {User} {Representation} {Learning} from {Command} {Sequences}},
	shorttitle = {{SimCURL}},
	doi = {10.1109/ICMLA55696.2022.00186},
	abstract = {User modeling is crucial to understanding user behavior and essential for improving user experience and personalized recommendations. When users interact with software, vast amounts of command sequences are generated through logging and analytics systems. These command sequences contain clues to the users' goals and intents. However, these data modalities are highly unstructured and unlabeled, making it difficult for standard predictive systems to learn from. We propose SimCURL, a simple yet effective contrastive self-supervised deep learning framework that learns user representation from unlabeled command sequences. Our method introduces a user-session network architecture, as well as session dropout as a novel way of data augmentation. We train and evaluate our method on a real-world command sequence dataset of more than half a billion commands. Our method shows significant improvement over existing methods when the learned representation is transferred to downstream tasks such as experience and expertise classification.},
	booktitle = {Proceedings of 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Chu, Hang and Khasahmadi, Amir Hosein and Willis, Karl D. D. and Anderson, Fraser and Mao, Yaoli and Tran, Linh and Matejka, Justin and Vermeulen, Jo},
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\fqixi\\Zotero\\storage\\74A949J6\\Chu et al. - 2022 - SimCURL Simple Contrastive User Representation Le.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fqixi\\Zotero\\storage\\FH29Y7AB\\2207.html:text/html},
}


@misc{pancha_pinnerformer_2022,
	title = {{PinnerFormer}: {Sequence} {Modeling} for {User} {Representation} at {Pinterest}},
	shorttitle = {{PinnerFormer}},
	url = {http://arxiv.org/abs/2205.04507},
	doi = {10.48550/arXiv.2205.04507},
	abstract = {Sequential models have become increasingly popular in powering personalized recommendation systems over the past several years. These approaches traditionally model a user's actions on a website as a sequence to predict the user's next action. While theoretically simplistic, these models are quite challenging to deploy in production, commonly requiring streaming infrastructure to reflect the latest user activity and potentially managing mutable data for encoding a user's hidden state. Here we introduce PinnerFormer, a user representation trained to predict a user's future long-term engagement using a sequential model of a user's recent actions. Unlike prior approaches, we adapt our modeling to a batch infrastructure via our new dense all-action loss, modeling long-term future actions instead of next action prediction. We show that by doing so, we significantly close the gap between batch user embeddings that are generated once a day and realtime user embeddings generated whenever a user takes an action. We describe our design decisions via extensive offline experimentation and ablations and validate the efficacy of our approach in A/B experiments showing substantial improvements in Pinterest's user retention and engagement when comparing PinnerFormer against our previous user representation. PinnerFormer is deployed in production as of Fall 2021.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Pancha, Nikil and Zhai, Andrew and Leskovec, Jure and Rosenberg, Charles},
	month = may,
	year = {2022},
	note = {arXiv:2205.04507 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\fqixi\\Zotero\\storage\\5PK4KQBH\\Pancha et al. - 2022 - PinnerFormer Sequence Modeling for User Represent.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fqixi\\Zotero\\storage\\S7TJGVGB\\2205.html:text/html},
}

@inproceedings{koren2009collaborative,
  title={Collaborative filtering with temporal dynamics},
  author={Koren, Yehuda},
  booktitle={Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={447--456},
  year={2009}
}

@inproceedings{he2016vista,
  title={Vista: A visually, socially, and temporally-aware model for artistic recommendation},
  author={He, Ruining and Fang, Chen and Wang, Zhaowen and McAuley, Julian},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={309--316},
  year={2016}
}

@inproceedings{he2016fusing,
  title={Fusing similarity models with markov chains for sparse sequential recommendation},
  author={He, Ruining and McAuley, Julian},
  booktitle={2016 IEEE 16th international conference on data mining (ICDM)},
  pages={191--200},
  year={2016},
  organization={IEEE}
}

@inproceedings{rendle2010factorizing,
  title={Factorizing personalized markov chains for next-basket recommendation},
  author={Rendle, Steffen and Freudenthaler, Christoph and Schmidt-Thieme, Lars},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={811--820},
  year={2010}
}

@inproceedings{beutel2018latent,
  title={Latent cross: Making use of context in recurrent recommender systems},
  author={Beutel, Alex and Covington, Paul and Jain, Sagar and Xu, Can and Li, Jia and Gatto, Vince and Chi, Ed H},
  booktitle={Proceedings of the eleventh ACM international conference on web search and data mining},
  pages={46--54},
  year={2018}
}

@inproceedings{hidasi2018recurrent,
  title={Recurrent neural networks with top-k gains for session-based recommendations},
  author={Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
  booktitle={Proceedings of the 27th ACM international conference on information and knowledge management},
  pages={843--852},
  year={2018}
}

@inproceedings{liu2016context,
  title={Context-aware sequential recommendation},
  author={Liu, Qiang and Wu, Shu and Wang, Diyi and Li, Zhaokang and Wang, Liang},
  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
  pages={1053--1058},
  year={2016},
  organization={IEEE}
}

@inproceedings{ren2019lifelong,
  title={Lifelong sequential modeling with personalized memorization for user response prediction},
  author={Ren, Kan and Qin, Jiarui and Fang, Yuchen and Zhang, Weinan and Zheng, Lei and Bian, Weijie and Zhou, Guorui and Xu, Jian and Yu, Yong and Zhu, Xiaoqiang and others},
  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={565--574},
  year={2019}
}

@inproceedings{pi2019practice,
  title={Practice on long sequential user behavior modeling for click-through rate prediction},
  author={Pi, Qi and Bian, Weijie and Zhou, Guorui and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2671--2679},
  year={2019}
}